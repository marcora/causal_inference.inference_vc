[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An intro to causal inference and do-calculus",
    "section": "",
    "text": "Preface\nThis “book” is inspired by (i.e., shamelessly ripped from) this 4-part series on causal inference written by Ferenc Huszár.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part_1.html",
    "href": "part_1.html",
    "title": "1  Introduction to causal inference and do-calculus",
    "section": "",
    "text": "https://www.inference.vc/untitled\nFirst of all, causal inference differentiates between two types of conditional distributions one might want to derive from a joint distribution \\(p(x, y, z, \\ldots)\\):\n\\[\np(y \\mid x)\n\\]\n\\[\np(y \\mid \\text{do}(x))\n\\]\nThe first is the distribution of \\(y\\) given that I observe \\(X = x\\), i.e., the observational conditional distribution.\nThe second is the distribution of \\(y\\) given that I assign the value \\(x\\) to \\(X\\) (\\(X := x\\)), i.e., the interventional conditional distribution.\n\\(p(y \\mid x)\\) and \\(p(y \\mid \\text{do}(x))\\) are not generally the same, and you can verify this with a simple thought experiment.\nLet’s say we jointly observe \\(X\\) (ice cream sales) and \\(Y\\) (shark attacks). They are correlated/statistically dependent and therefore seeing \\(X = x\\) allows me to predict the value of \\(Y\\), but \\(Y\\) is not caused by \\(X\\) and therefore setting the value of \\(X\\) to \\(x\\) won’t affect the distribution of \\(Y\\). In other words:\n\\[\np(y \\mid x) \\ne p(y)\n\\]\n\\[\np(y \\mid \\text{do}(x)) = p(y)\n\\]\nDepending on the purpose of your statistical model, you should seek to estimate one of these conditionals. If your ultimate goal is diagnosis or forecasting (i.e. observing a naturally occurring \\(x\\) and inferring the probable values of $y$) you want the observational conditional \\(p(y \\mid x)\\). This is what we already do in supervised learning and regression analysis. But if \\(x\\) is a medical treatment and \\(y\\) is the disease outcome, you are not merely interested in diagnosis or forecasting. You are interested in inferring the effect of administering \\(x\\) on \\(y\\).\n\\(p(y \\mid \\text{do}(x))\\) is in fact a vanilla conditional distribution, but it’s not computed based on the joint distribution \\(p(x, y, z, \\ldots)\\), but rather based on a modified joint distribution \\(p'(x, y, z, \\ldots)\\) where the mechanism that generates \\(X\\) has been replaced by a mechanism that always outputs \\(x\\). This is the joint distribution which we would observe if we actually carried out the intervention in question. \\(p(y \\mid \\text{do}(x))\\) is the conditional distribution we would learn from data collected in an experiment like a randomized controlled trial or A/B test where the experimenter controls x. Note that actually carrying out the intervention or experiment may be impossible or at least impractical or unethical in many situations.\nThe main point of causal inference and do-calculus is: If I cannot estimate \\(p(y \\mid \\text{do}(x))\\) directly with a randomized controlled trial, can I estimate it based on data I observed outside of a controlled experiment?\nXXX\nComing up with a causal model is a modeling step where we have to consider assumptions about how the world works, what causes what. Once we have a causal diagram, we can emulate the effect of intervention by mutilating the causal graph: deleting all edges that lead into the node in the \\(\\text{do}\\) operator.\nThe mapping from causal diagrams to joint distributions is many-to-one: several causal diagrams are compatible with the same joint distribution. Thus, it is generally impossible to conclusively choose between different causal explanations by looking at observed data only.\nDo-calculus extends probability theory with four additional rules we can apply to conditional distributions with the \\(\\text{do}\\) operator in them. These rules take into account properties of the causal diagram. Here is an introductory paper on them.\nIdeally, as a result of a do-calculus derivation you end up with an equivalent formula for \\(p(y \\mid \\text{do}(x))\\) which no longer has any \\(\\text{do}\\) operators in them, so you estimate it from observational data alone. If this is the case we say that the causal query \\(p(y \\mid \\text{do}(x))\\) is identifiable. Conversely, if this is not possible, no matter how hard we try applying do-calculus, we call the causal query non-identifiable, which means that we won’t be able to estimate it from the data we have.\nYou can never fully verify the validity and completeness of your causal diagram based on observed data alone. However, there are certain aspects of the causal model which are empirically testable. In particular, the causal diagram implies certain conditional independence or dependence relationships between sets of variables. These dependencies or independencies can be empirically tested, and if they are not present in the data, that is an indication that your causal model is wrong. Taking this idea forward you can attempt to perform full causal discovery: attempting to infer the causal model or at least aspects of it, from empirical data.\nBut the bottom line is: a full causal model is a form of prior knowledge that you have to add to your analysis in order to get answers to causal questions without actually carrying out interventions. Reasoning with data alone won’t be able to give you this. Unlike priors in Bayesian analysis - which are a nice-to-have and can improve data-efficiency - causal diagrams in causal inference are a must-have. With a few exceptions, all you can do without them is running randomized controlled experiments.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to causal inference and do-calculus</span>"
    ]
  },
  {
    "objectID": "part_2.html",
    "href": "part_2.html",
    "title": "2  Illustrating interventions with toy simulations",
    "section": "",
    "text": "https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/\n\n\nlibrary(tidyverse)\nlibrary(ggformula)\n\ntheme_set(theme_bw())\n\nn &lt;- 10000\ndelta &lt;- 0.5\n\n\nset.seed(1)\n\n# Panel A (red)\ndf1 &lt;- tibble(\n  x = rnorm(n),\n  y = x + 1 + sqrt(3) * rnorm(n),\n  panel = \"A\"\n)\n\n# Panel B (green)\ndf2 &lt;- tibble(\n  y = 1 + 2 * rnorm(n),\n  x = (y - 1) / 4 + sqrt(3) * rnorm(n) / 2,\n  panel = \"B\"\n)\n\n# Panel C (blue)\ndf3 &lt;- tibble(\n  z = rnorm(n),\n  y = z + 1 + sqrt(3) * rnorm(n),\n  x = z,\n  panel = \"C\"\n)\n\ndf_obs &lt;- bind_rows(df1, df2, df3)\n\ngf_point(y ~ x | panel, data = df_obs, alpha = 0.05, color = ~ panel)\n\n\n\n\n\n\n\n\nAs you can see, all three scripts produce the same joint distribution between \\(x\\) and \\(y\\). You can feed these distributions into a two-sample test, and you will find that they are indeed indistinguishable from each other.\n\ngf_point(y ~ x | panel, data = df_obs, alpha = 0.05, color = ~ panel) |&gt; gf_lm()\n\n\n\n\n\n\n\n\nBut despite the three scripts being equivalent in that they generate the same joint distribution between \\(x\\) and \\(y\\), they are not the same. For example, they behave differently if we interfere or intervene to the execution.\n\nset.seed(1)\n\n# Panel A (red)\ndf1 &lt;- tibble(\n  x = 3, # replace causal mechanism of X to always output 3, i.e., intervene to set X = 3\n  y = x + 1 + sqrt(3) * rnorm(n),\n  panel = \"A\"\n)\n\n# Panel B (green)\ndf2 &lt;- tibble(\n  y = 1 + 2 * rnorm(n),\n  x = 3, # replace causal mechanism of X to always output 3, i.e., intervene to set X = 3\n  panel = \"B\"\n)\n\n# Panel C (blue)\ndf3 &lt;- tibble(\n  z = rnorm(n),\n  y = z + 1 + sqrt(3) * rnorm(n),\n  x = 3, # replace causal mechanism of X to always output 3, i.e., intervene to set X = 3\n  panel = \"C\"\n)\n\ndf_int &lt;- bind_rows(df1, df2, df3)\n\ngf_point(y ~ x | panel, data = df_int, alpha = 0.005, color = ~ panel)\n\n\n\n\n\n\n\n\nOf course, we see that the value of x is no longer random, it’s deterministically set to 3, this results in all samples lining up along the x=3 vertical line. But, interestingly, the distribution of y is different for the different scripts. In the blue script, y has a mean around 5 while the green and red scripts produce a distribution of y centered around a mean of 1. Here is a better look at the marginal distribution of y under the intervention:\n\ngf_density(~ y, data = df_int, alpha = 0.5, fill = ~ panel)\n\n\n\n\n\n\n\n\nThis is generally different from the conditional distribution p(y|x=3), which of course is the same for all three scripts.\n\nset.seed(1)\n\n# Panel A (red)\ndf1 &lt;- tibble(\n  x = rnorm(n),\n  y = x + 1 + sqrt(3) * rnorm(n),\n  panel = \"A\"\n) |&gt; filter(abs(x - 3) &lt; delta) # condition on X=3\n\n# Panel B (green)\ndf2 &lt;- tibble(\n  y = 1 + 2 * rnorm(n),\n  x = (y - 1) / 4 + sqrt(3) * rnorm(n) / 2,\n  panel = \"B\"\n) |&gt; filter(abs(x - 3) &lt; delta) # condition on X=3\n\n# Panel C (blue)\ndf3 &lt;- tibble(\n  z = rnorm(n),\n  y = z + 1 + sqrt(3) * rnorm(n),\n  x = z,\n  panel = \"C\"\n) |&gt; filter(abs(x - 3) &lt; delta) # condition on X=3\n\ndf_cnd &lt;- bind_rows(df1, df2, df3)\n\ngf_density(~ y, data = df_cnd, alpha = 0.5, fill = ~ panel) + expand_limits(x = c(-4, 12))\n\n\n\n\n\n\n\n\nThe important point here is that the scripts are indistinguishable when you only look at the joint distribution of the samples they produce, yet they behave differently under intervention.\nConsequently, the joint distribution of data alone is insufficient to predict the effects of interventions.\nIf the joint distribution is insufficient, what level of description would allow us to make predictions about how the scripts behave under intervention. If I have the full source code, I can of course execute the modified scripts, i.e. run an experiment and directly observe how the interaction effects the distribution.\nHowever, it turns out, you don’t need the full source code. It is sufficient to know the causal diagram corresponding to the source code. The causal diagram encodes causal relationships between variables, with an arrow pointing from causes to effects. Here is what the causal diagrams would look like for these scripts:\n\nWe can see that, even though they produce the same joint distribution, the scripts have different causal diagrams. And this additional knowledge of the causal structure allows us to make inferences about intervention without actually running experiments with that intervention. To do this, we can use do-calculus.\nGraphically, to simulate the effect of an intervention, you mutilate the graph by removing all edges that point into the variable on which the intervention is applied, in this case \\(x\\).\n\n\nset.seed(1)\n\n# Panel A (red)\ndf1 &lt;- tibble(\n  x = rnorm(n),\n  y = x + 1 + sqrt(3) * rnorm(n),\n  panel = \"A\"\n) |&gt; filter(abs(x - 3) &lt; delta) # condition on X=3 to match the intervention\n\n# Panel B (green)\ndf2 &lt;- tibble(\n  y = 1 + 2 * rnorm(n),\n  x = (y - 1) / 4 + sqrt(3) * rnorm(n) / 2,\n  panel = \"B\"\n) # do NOT condition on X=3 to match the intervention\n\n# Panel C (blue)\ndf3 &lt;- tibble(\n  z = rnorm(n),\n  y = z + 1 + sqrt(3) * rnorm(n),\n  x = z,\n  panel = \"C\"\n) # do NOT condition on X=3 to match the intervention\n\ndf_int_from_obs &lt;- bind_rows(df1, df2, df3)\n\ngf_density(~ y, data = df_int_from_obs, alpha = 0.5, fill = ~ panel)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Illustrating interventions with toy simulations</span>"
    ]
  },
  {
    "objectID": "part_3.html",
    "href": "part_3.html",
    "title": "3  Counterfactuals",
    "section": "",
    "text": "https://www.inference.vc/causal-inference-3-counterfactuals/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Counterfactuals</span>"
    ]
  },
  {
    "objectID": "part_4.html",
    "href": "part_4.html",
    "title": "4  Causal diagrams and structural equation models",
    "section": "",
    "text": "https://www.inference.vc/causal-inference-4/",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Causal diagrams and structural equation models</span>"
    ]
  }
]